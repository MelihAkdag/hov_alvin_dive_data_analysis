{"cells":[{"metadata":{},"cell_type":"markdown","source":"# HOV ALVIN DIVE LOGS\n\nWoods Hole Institute's (WHOI) Human Occupied Vehicle HOV ALVIN allows in-situ data collection and observation by two scientists of the seafloor and water column to depths reaching 4,500 meters on dives lasting up to ten hours (https://ndsf.whoi.edu/alvin/).\n\nIn 2018, when I've started to learn data science through online courses, I used Woods Hole Institute's (WHOI) Human Occupied Vehicle (HOV) Alvin's dive logs for exploratory data analysis (EDA) and visualisation practices.\n\nThe data which I studied back then has Alvin's dive logs up to 2017. One can reach that data set through: https://www.kaggle.com/sauuyer/alvin-dives\n\nSo I decided to use my **web scraping** and **data preparation** skills to update the data set.\n\nOne can examine WHOI's data through this link: https://ndsf.whoi.edu/alvin/dive-log/\n\n## SKILLS USED IN THIS NOTEBOOK:\n\n* Web scraping; which means using Python and Beautiful Soup library to collect the data from a web site,\n* Data cleaning and data preparation skills,\n* Data visualization skills for graphs and analyze the data,\n* Geospatial visualization skills to demonstrate dive destinations on the map.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/hov-alvin-dive-log/hov_alvin.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Importing necessary libraries:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\nimport requests\nimport datetime\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. ALVIN'S DIVE LOG BY PILOTS TABLE:\n\nFirst I am going to scrape the dives by pilot table.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the url where the data is stored:\nurl_list = ['http://dsg.whoi.edu/divelog.nsf/By%20Pilot%20Name?OpenView&Start=1',\n            'http://dsg.whoi.edu/divelog.nsf/By%20Pilot%20Name?OpenView&Start=5065&Count=6000']\n\n# Create an empty list for storage:\ndata1 = []\n\n# Using for loops to collects columns of data from the urls:\nfor each in url_list:\n    r = requests.get(each)\n    soup = BeautifulSoup(r.content, 'html.parser')\n    \n    for tr in soup.find_all('tr'):\n        col = 0\n        for td in tr.find_all('td'):\n            td_text = td.get_text().strip()\n            if col == 0:\n                pilot = td_text\n            if col == 1:\n                dive_nu = td_text\n            if col == 2:\n                date = td_text\n            if col == 3:\n                op_area = td_text\n            if col == 4:\n                lat = td_text\n            if col == 5:\n                lon = td_text\n            if col == 6:\n                depth = td_text\n            if col == 7:\n                obs1 = td_text\n            if col == 8:\n                obs2 = td_text\n            if col == 9:\n                dive_time = td_text\n            if col == 10:\n                bottom_time = td_text if td_text else np.nan\n        \n                data1.append({'pilot': pilot,\n                             'dive_nu': dive_nu,\n                             'date': date,\n                             'op_area': op_area,\n                             'lat': lat,\n                             'lon': lon,\n                             'depth': depth,\n                             'obs1': obs1,\n                             'obs2': obs2,\n                             'dive_time': dive_time,\n                             'bottom_time': bottom_time})\n            col += 1\n\n# Turn collected data into Pandas Dataframe: \ndf1 = pd.DataFrame(data1, columns=['pilot', 'dive_nu', 'date', 'op_area', \n                                   'lat', 'lon', 'depth', 'obs1', 'obs2', 'dive_time', 'bottom_time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Erasing duplicated values:\ndf1.drop_duplicates(subset='dive_nu', keep='first' , inplace=True)\nprint('Is there any duplicated value? ', df1.duplicated().sum())\n\n# Let's see what we have collected:\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. ALVIN'S DIVE LOG SUMMARY TABLE:\n\nIn the summary table we can find the information about the chief scientists, cruise and cruise leg numbers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Urls for the summary table:\nurl_list = ['http://dsg.whoi.edu/divelog.nsf/Summary?OpenView&Start=1', \n            'http://dsg.whoi.edu/divelog.nsf/Summary?OpenView&Start=5065']\n\n# Create an empty list for storage:\ndata2 = []\n\n# Collect data by for loops:\nfor each in url_list:\n    r = requests.get(each)\n    soup = BeautifulSoup(r.content, 'html.parser')\n    \n    for tr in soup.find_all('tr'):\n        col = 0\n        for td in tr.find_all('td'):\n            td_text = td.get_text().strip()\n            if col == 0:\n                date = td_text\n            if col == 1:\n                dive_nu = td_text\n            if col == 2:\n                cruise = td_text\n            if col == 3:\n                leg = td_text\n            if col == 4:\n                chief_sci = td_text\n                \n                data2.append({'date': date,\n                              'dive_nu': dive_nu,\n                              'cruise': cruise,\n                              'leg': leg,\n                              'chief_sci': chief_sci})\n            col += 1\n\n# Convert the data into Pandas Dataframe:\ndf2 = pd.DataFrame(data2, columns=['date', 'dive_nu', 'cruise', 'leg', 'chief_sci'])\n\n# Erase the first row from the dataset which is the website menu items: \ndf2 = df2[1:5066]\n\n# Let's see what we have collected:\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. BY DIVE NUMBER/DATE TABLE:\n\nIn this table we can find the information about purpose of the each dive.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Urls for the summary table:\nurl_list = ['http://dsg.whoi.edu/divelog.nsf/By%20Dive%20Number/Date?OpenView',\n            'http://dsg.whoi.edu/divelog.nsf/By%20Dive%20Number/Date?OpenView&Start=5065']\n\ndata3 = []\n\n# Collecting data from the web site:\nfor each in url_list:\n    r = requests.get(each)\n    soup = BeautifulSoup(r.content, 'html.parser')\n    \n    for tr in soup.find_all('tr'):\n        col = 0\n        for td in tr.find_all('td'):\n            td_text = td.get_text().strip()\n            if col == 0:\n                dive_nu = td_text\n            if col == 1:\n                date = td_text\n            if col == 2:\n                purpose = td_text\n                \n                data3.append({'dive_nu': dive_nu, 'date': date, 'purpose': purpose})\n            col += 1\n\n# Converting data into Pandas Dataframe:            \ndf3 = pd.DataFrame(data3, columns=['dive_nu', 'date', 'purpose'])\n\n# Erase website menu items from the dataset \ndf3 = df3[1:5066]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see our three datasets:\ndisplay(df1.head(2))\ndisplay(df2.head(2))\ndisplay(df3.head(2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. COMBINING TABLES:\n\nI am going to merge three tables on the date of the dive and the dive number columns, in order to create my final dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the datasets:\ndata_raw = pd.merge(df1, df2, on=['date', 'dive_nu'], how='outer')\ndata_raw = pd.merge(data_raw, df3, on=['date', 'dive_nu'], how='outer')\n\n# Changing the order of the columns:\ndata_raw = data_raw[['dive_nu', 'date', 'op_area', 'lat', 'lon', 'cruise', \n                     'leg', 'purpose', 'depth', 'dive_time', 'bottom_time',\n                     'chief_sci', 'pilot', 'obs1', 'obs2']]\n\ndisplay(data_raw.head(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I am going to save the raw data before any data preparation. So one can use it to practice for data preparation and cleaning skills.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# data_raw.to_csv(r'/Users/melihakdag/Desktop/Data Science/alvin_dive_logs/alvin_data_raw.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. DATA CLEANING AND PREPARATION:\n\nNow I have collected my data. But I need to examine my data for data types and prepare some of the columns for later analysis.\n\nFor example I want my latitude and longitude columns in decimal degrees for geospatial visualization. I want the dive_time column as minutes instead of hour & minutes style. I want the dive number and depth column types as integer.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_raw.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing 'date' column type into datetime:\ndata_raw['date'] = pd.to_datetime(data_raw['date'])\ndata_raw.tail(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen from above the date 64 (1964) converted into 2064. \n\nSo we need to use a function to correct the years:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the function:\ndef change_year(x):\n    if x.year > 2060:\n        year = x.year - 100\n        \n    else:\n        year = x.year\n\n    return datetime.date(year,x.month,x.day)\n\n# Apply the function:\ndata_raw['date'] = data_raw['date'].apply(change_year)\n\n# Let's see if the dates are correct now:\ndata_raw.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing 'dive_time' and 'bottom_time' column types into minutes:\ndef minutes(x):\n    if type(x) == str:\n        return int(x[:-3])*60 + int(x[-2:])\n    else:\n        return np.nan\n        \ndata_raw['bottom_time'] = data_raw['bottom_time'].apply(minutes)\ndata_raw['dive_time'] = data_raw['dive_time'].apply(minutes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing 'dive_nu', 'depth' columns' types into integer:\nto_integer = lambda x: int(x)\n\ncolumns = ['dive_nu', 'depth']\n\nfor each in columns:\n    data_raw[each] = data_raw[each].apply(to_integer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to prepare the latitude and longitude columns:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting latitude and longitude strings by '-':\ndata_raw[['lat01', 'lat02']] = data_raw['lat'].str.split(pat=\"-\", expand=True)\ndel data_raw['lat']\n\ndata_raw[['long01', 'long02']] = data_raw['lon'].str.split(pat='-', expand=True)\ndel data_raw['lon']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_raw.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see if there is any missing values in the new features:\ndata_raw[['lat01', 'lat02', 'long01', 'long02']].isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Which columns have the missing values? \nprint('lat01 any NaN : ', data_raw['lat01'].isnull().values.any())\nprint('lat02 any NaN : ', data_raw['lat02'].isnull().values.any())\nprint('long01 any NaN: ', data_raw['long01'].isnull().values.any())\nprint('long02 any NaN: ', data_raw['long02'].isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find those missing values in long02 column:\ndata_raw[data_raw['long02'].isnull() == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It looks like the missing values are caused by a typo. Let's fix the typos: \ndata_raw.iloc[3508, 15] = data_raw.iloc[3508, 15].replace('95.28.5W', '95')\ndata_raw.iloc[3508, 16] = '28.5W'\n\ndata_raw.iloc[3514, 15] = data_raw.iloc[3514, 15].replace('95.33.0W', '95')\ndata_raw.iloc[3514, 16] = '33.0W'\n\ndata_raw.iloc[[3508, 3514]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing latidude and longitude strings into numbers:\ndata_raw['lat01'] = data_raw['lat01'].apply(lambda x: float(x))\ndata_raw['long01'] = data_raw['long01'].apply(lambda x: float(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing latitude and longitude degree signs according to being on the Southern hemisphere or having a Western longitude:\ndef change_sign(x, y):\n    if y[-1] == 'S' or y[-1] == 'W':\n        return x*-1\n    else:\n        return x\n    \ndata_raw['lat01'] = data_raw.apply(lambda x: change_sign(x.lat01, x.lat02), axis=1)\ndata_raw['long01'] = data_raw.apply(lambda x: change_sign(x.long01, x.long02), axis=1)\n\ndata_raw.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing Decimal Minutes to Decimal Degrees:\ndef to_DD(x):\n    if x[-1:] == 'N':\n        return float(x[:-1])/60\n    if x[-1:] == 'S':\n        return float(x[:-1])/-60\n    if x[-1:] == 'E':\n        return float(x[:-1])/60\n    if x[-1:] == 'W':\n        return float(x[:-1])/-60\n\n# Finding Decimal Degrees:\ndata_raw['lat02'] = data_raw['lat02'].apply(to_DD)\ndata_raw['long02'] = data_raw['long02'].apply(to_DD)\n\n# Saving the DD latidude and longitudes and deleting other coordinate datas:\ndata_raw['lat(DD)'] = data_raw['lat01'] + data_raw['lat02']\ndata_raw['long(DD)'] = data_raw['long01'] + data_raw['long02']\ndata_raw.drop(['lat01', 'lat02', 'long01', 'long02'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Strings in the columns are written in different formats like upper case, lower case, all capitals.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data inputs are written in different styles:\ndata_raw[['op_area','purpose', 'chief_sci']].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's make the string styles all the same:\ndata_raw[['op_area','purpose', 'chief_sci']] = data_raw[['op_area','purpose', 'chief_sci']].apply(lambda x: x.str.title())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a copy of the final data set:\nalvin_dives = data_raw.copy()\n\n# Finally we can export this cleaned dataset:\n#alvin_dives.to_csv(r'/Users/melihakdag/Desktop/Data Science/alvin_dive_logs/alvin_data_cleaned.csv')\n\nalvin_dives.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. VISUALIZATION \n\nNow we have a clean and an updated ALVIN dive log dataset, we can visualize and examine the dataset in order to extract some insights. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.express as px\nimport folium\nfrom folium import Circle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1. ALVIN's dive destinations:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the borders of our map:\nsw = alvin_dives[['lat(DD)', 'long(DD)']].min().values.tolist()\nne = alvin_dives[['lat(DD)', 'long(DD)']].max().values.tolist()\n\n# Creating the basemap:\nworld_map = folium.Map(zoom_start = 9)\n# Default zoom setting with the boundaries:\nworld_map.fit_bounds([sw, ne])\n\n# Creating circle marks on the basemap:\nfor lat, long, date, depth in zip(alvin_dives['lat(DD)'], alvin_dives['long(DD)'], \n                                       alvin_dives['date'], alvin_dives['depth']): \n    folium.Circle(location = [lat, long], \n                  radius = 20).add_child(folium.Popup(str(date.year) + ', ' + str(depth) + ' m')).add_to(world_map)\n    \nworld_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2. ALVIN's dive history (Dives & Dates Distribution):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see total number of dives for each year.\n# Total dives for the first ten years:\ndive_years = pd.DataFrame(alvin_dives.groupby(alvin_dives['date'].map(lambda x: x.year)).dive_nu.count())\ndive_years.reset_index(inplace=True)\ndive_years.rename(columns={'date':'year', 'dive_nu':'total_dive_nu'}, inplace=True)\ndive_years.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.line(dive_years, \n              x=\"year\", \n              y=\"total_dive_nu\", \n              title='Total Number of Dives in Years')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3. Scientific purposes of the dives: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many different purposes are there?\nprint('Alvin have dived for %d different purposes.' %len(alvin_dives['purpose'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the distribution of the purposes:\npurpose_count = pd.DataFrame(alvin_dives['purpose'].value_counts())\npurpose_count.reset_index(inplace=True)\npurpose_count.rename(columns = {'index':'purpose', 'purpose':'total_nu'}, inplace=True)\npurpose_count.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.bar(purpose_count.head(20),\n             x='purpose',\n             y='total_nu',\n             title='Top 20 Purposes of the Dives')\n\nfig.update_traces(marker=dict(color=\"RoyalBlue\"))\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4. Depths of the dives:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dive_depths = alvin_dives[['date', 'dive_nu', 'depth', 'dive_time']]\ndive_depths['depth'] = dive_depths['depth'].map(lambda x: -(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.scatter(dive_depths,\n                 x = 'date', \n                 y = 'depth',\n                 title = \"Alvin's Dive Dates & Depths & Times\",\n                 color='dive_time',\n                 hover_data = ['dive_nu'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.5. Depths and the purposes of the dives:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by purpose and find the average depth for each purpose:\npurpose_avg_depth = pd.DataFrame(alvin_dives.groupby(['purpose']).depth.median())\npurpose_avg_depth.reset_index(inplace=True)\npurpose_avg_depth.rename(columns={'depth':'avg_depth'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine the purpose count and the average depth tables:\npurpose_depth_count = pd.merge(purpose_count, purpose_avg_depth, on=['purpose'])\npurpose_depth_count['avg_depth'] = purpose_depth_count['avg_depth'].map(lambda x: -(x))\npurpose_depth_count.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.scatter(purpose_depth_count.head(20), \n                 x = 'purpose',\n                 y = 'avg_depth',\n                 size = 'total_nu',\n                 title = 'Average Depths for Purposes')   \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.6. Depths and Maximum Dive times:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_dive_time = pd.DataFrame(alvin_dives.groupby(['depth'])['dive_time'].max())\nmax_dive_time.reset_index(inplace=True)\nmax_dive_time.rename(columns={'dive_time': 'max_dive_time'}, inplace=True)\nmax_dive_time['depth'] = max_dive_time['depth'].map(lambda x: -(x)) \nmax_dive_time.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.scatter(max_dive_time, \n                 x = 'max_dive_time',\n                 y = 'depth',\n                 color = 'depth',\n                 title = 'Maximum Dive Times for Depths')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.7. ALVIN's pilots:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's extract the pilots, chief scientists, observers,\ndivers = alvin_dives[['pilot', 'chief_sci', 'obs1', 'obs2', 'date', 'dive_nu', 'purpose']]\ndivers['date'] = divers['date'].map(lambda x: x.year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('%d talented people had chance to work as a pilot for ALVIN since 1964.' %len(divers['pilot'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"divers_dives = pd.DataFrame(divers.groupby(['pilot', 'date']).dive_nu.count())\ndivers_dives.reset_index(inplace=True)\ndivers_dives.rename(columns={'date': 'year', 'dive_nu': 'total_dives'}, inplace=True)\ndivers_dives.sort_values(by='year', inplace=True)\ndivers_dives.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.bar(divers_dives, \n             y = 'pilot',\n             x = 'total_dives',\n             color = 'year',\n             title = 'Pilots and Total Dive Numbers')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.scatter(divers_dives, \n                 y = 'pilot',\n                 x = 'year',\n                 color = 'year',\n                 title = 'Pilots and Years')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.8. Chief Scientists:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"chief_scientist = pd.DataFrame(divers.groupby(['chief_sci', 'date']).dive_nu.count())\nchief_scientist.reset_index(inplace=True)\nchief_scientist.rename(columns={'date': 'year', 'dive_nu': 'total_dives'}, inplace=True)\nchief_scientist.sort_values(by='year', ascending=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"chief_scientist.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.scatter(chief_scientist,\n                 x = 'year',\n                 y = 'chief_sci',\n                 color = 'year',\n                 size = 'total_dives',\n                 title = 'Chief Scientists and Research Years')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.9. Cruises and Dive Numbers:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cruises and total number of dives:\ncruise_dives = pd.DataFrame(alvin_dives.groupby('cruise').dive_nu.count().sort_values(ascending=False))\ncruise_dives.rename(columns={'dive_nu':'total_dives'}, inplace=True)\n\n# Getting the cruises which have more than 50 dives:\ncruise_dives50 = cruise_dives[cruise_dives.values > 50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.pie(cruise_dives50,\n             names = cruise_dives50.index,\n             values = 'total_dives',\n             title = 'Cruises With More Than 50 Dives')\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}